#!/usr/bin/env python3
from argparse import ArgumentParser
from collections import OrderedDict
from csv import DictReader, writer
from dataclasses import asdict, dataclass, fields
from datetime import datetime, timedelta
from enum import Enum
from os.path import join
from random import Random
from scapy.layers.inet import IP
from scapy.layers.kerberos import KRB_TGS_REQ, EncryptedData, KRB_Ticket, KRB_AP_REQ, PADATA, KRB_KDC_REQ_BODY, \
    KRB_AS_REQ
from scapy.packet import Packet
from scapy.sendrecv import sniff
from schedule import every, run_pending
from signal import signal, SIGTERM
from sklearn.metrics import *
from sklearn.neighbors import LocalOutlierFactor
from sklearn.svm import OneClassSVM
from syslog import LOG_WARNING, syslog, LOG_INFO
from threading import Thread
from time import perf_counter, sleep


@dataclass
class Record:
    time: datetime
    type: str
    source: str
    cname: str | None
    sname: str
    realm: str
    mark: int | None

@dataclass
class FrequenciesItem:
    number: int
    source_sname_pairs: set[tuple[str, str]]
    attack_marks: int


class LearningMechanism(Enum):
    LOF = 1
    SVM = 2

    def __str__(self):
        return self.name


rc4_hmac_signature = 23
random = Random()
lof_min = -1.1
records: list[Record]
frequencies: OrderedDict[datetime, FrequenciesItem]
running = True
mech: LearningMechanism
svm: OneClassSVM


def handle_sigterm(signum, frame):
    dump_dataset()
    global running
    running = False
    exit(0)


def run_schedule():
    while running:
        run_pending()
        sleep(1)


def dump_dataset():
    timestamp = int(datetime.now().timestamp())  # количество секунд с начала эпохи (01.01.1970)
    global records
    if len(records) > 0:
        with open(join('datasets', f'{timestamp}dataset.csv'), 'w') as dataset_file:
            csv_writer = writer(dataset_file)
            csv_writer.writerow([f.name for f in fields(Record)])
            record_fields = [[value for name, value in asdict(record).items()] for record in records]
            csv_writer.writerows(record_fields)
        request_number = len(records)
        records = []
        syslog(LOG_INFO, f'Сделан дамп запросов в файл {timestamp}dataset.csv '
                         f'(количество запросов: {request_number})')
    else:
        syslog(LOG_INFO, f'Дамп запросов не сделан, т.к. не было запросов')


def load_dataset():
    loaded_records = []
    with open(join('datasets', 'sample.csv')) as dataset_file:
        csv_reader = DictReader(dataset_file)
        for row in csv_reader:
            record = Record(**row)
            # необходимо преобразовать в datetime, т.к. csv_reader прочитает datetime как строку
            record.time = datetime.strptime(record.time, '%Y-%m-%d %H:%M:%S.%f')
            record.mark = 0 if record.mark == '' else int(record.mark)
            loaded_records.append(record)
    freqs = OrderedDict()
    for record in loaded_records:
        time_without_seconds = record.time.replace(microsecond=0)
        if time_without_seconds in freqs.keys():
            freqs[time_without_seconds].number += 1
            freqs[time_without_seconds].source_sname_pairs.add((record.source, record.sname))
            freqs[time_without_seconds].attack_marks += record.mark
        else:
            freqs[time_without_seconds] = FrequenciesItem(1, {(record.source, record.sname)}, record.mark)
    syslog(LOG_INFO, f'Датасет загружен')
    if mech == LearningMechanism.SVM:
        global svm
        svm = OneClassSVM(nu=1e-5, gamma=0.1)
        frequencies_vectors = [(item.number,
                                len(item.source_sname_pairs) / len(set([pair[0] for pair in item.source_sname_pairs])))
                               for item in freqs.values()]
        time_start = perf_counter()
        prediction = svm.fit_predict(frequencies_vectors)
        time_end = perf_counter()
        syslog(LOG_INFO, f'SVM обучена за {time_end - time_start} секунд')
        marks = [1 if item.attack_marks == 0 else -1 for item in freqs.values()]
        write_svm_learning_results(marks, prediction)
    return freqs


def write_svm_learning_results(marks: list[int], prediction: list[int]):
    with open('svm_metrics.txt', 'w') as svm_metrics:
        # Classification metrics
        print(f'Accuracy score: {accuracy_score(marks, prediction)}')
        print(f'Balanced accuracy score: {balanced_accuracy_score(marks, prediction)}')
        print(f'Top k accuracy: {top_k_accuracy_score(marks, prediction)}')  # k = 2 (бесполезно)
        print(f'Average precision score: {average_precision_score(marks, prediction)}')
        # print(f'Brier score loss: {brier_score_loss(marks, prediction)}') (не принимает отрицательные значения)
        print(f'F1 score: {f1_score(marks, prediction)}')
        print(f'F1 micro score: {f1_score(marks, prediction, average="micro")}')
        print(f'F1 macro score: {f1_score(marks, prediction, average="macro")}')
        print(f'F1 weighted score: {f1_score(marks, prediction, average="weighted")}')
        # print(f'F1 samples score: {f1_score(marks, prediction, average="samples")}') # Samplewise metrics are not available outside of multilabel classification.
        print(f'Negative log loss: {log_loss(marks, prediction)}')
        print(f'Precision score: {precision_score(marks, prediction)}')
        print(f'Recall score: {recall_score(marks, prediction)}')
        print(f'Jaccard score: {jaccard_score(marks, prediction)}')
        print(f'ROC AUC score: {roc_auc_score(marks, prediction)}')
        print(f'ROC AUC OVR score: {roc_auc_score(marks, prediction, multi_class="ovr")}')
        print(f'ROC AUC OVO score: {roc_auc_score(marks, prediction, multi_class="ovo")}')
        print(f'ROC AUC OVR weighted score: {roc_auc_score(marks, prediction, multi_class="ovr", average="weighted")}')
        print(f'ROC AUC OVO weighted score: {roc_auc_score(marks, prediction, multi_class="ovo", average="weighted")}')
        print(f'D2 log loss score: {d2_log_loss_score(marks, prediction)}')
        # Clustering metrics
        print(f'Adjusted mutual info score: {adjusted_mutual_info_score(marks, prediction)}')
        print(f'Adjusted rand score: {adjusted_rand_score(marks, prediction)}')
        print(f'Completeness score: {completeness_score(marks, prediction)}')
        print(f'Fowlkes-Mallows score: {fowlkes_mallows_score(marks, prediction)}')
        print(f'Homogeneity score: {homogeneity_score(marks, prediction)}')
        print(f'Mutual info score: {mutual_info_score(marks, prediction)}')
        print(f'Normalized mutual info score: {normalized_mutual_info_score(marks, prediction)}')
        print(f'Rand score: {rand_score(marks, prediction)}')
        print(f'V-measure score: {v_measure_score(marks, prediction)}')
        # Regression metrics
        print(f'Explained variance: {explained_variance_score(marks, prediction)}')
        print(f'Max error: {max_error(marks, prediction)}')
        print(f'Negative mean absolute error: {mean_absolute_error(marks, prediction)}')
        print(f'Negative mean squared error: {mean_squared_error(marks, prediction)}')
        print(f'Negative root mean squared error: {root_mean_squared_error(marks, prediction)}')
        # print(f'Negative mean squared log error: {mean_squared_log_error(marks, prediction)}') -1 с квадратами не 
        # print(f'Negative root mean squared log error: {root_mean_squared_log_error(marks, prediction)}') -//-
        print(f'Negative median absolute error: {median_absolute_error(marks, prediction)}')
        print(f'R2 score: {r2_score(marks, prediction)}')
        # print(f'Negative mean Poisson deviance: {mean_poisson_deviance(marks, prediction)}') -//-
        # print(f'Negative mean gamma deviance: {mean_gamma_deviance(marks, prediction)}') -//-
        print(f'Negative mean absolute percentage error: {mean_absolute_percentage_error(marks, prediction)}')
        print(f'D2 absolute error score: {d2_absolute_error_score(marks, prediction)}')
    syslog(LOG_INFO, 'Метрики обучения SVM записаны в файл svm_metrics.txt')


def find_anomaly():
    # если в течение последней секунды не зафиксировано никакой активности, то и искать аномалии бессмысленно
    time = datetime.now()
    if (time.replace(microsecond=0) - timedelta(seconds=1)) not in frequencies.keys():
        return
    frequencies_vectors = [(item.number, len(item.source_sname_pairs)) for item in frequencies.values()]
    if mech == LearningMechanism.LOF:
        lof = LocalOutlierFactor(n_neighbors=20, contamination=0.1)
        time_start = perf_counter()
        lof.fit_predict(frequencies_vectors)
        lof_values = lof.negative_outlier_factor_
        time_stop = perf_counter()
        last_measurement = lof_values[-1]
        if last_measurement < lof_min:
            syslog(LOG_WARNING, f'Обнаружено аномально большое количество пакетов. '
                                f'Время анализа: {time_stop - time_start} секунд')
    elif mech == LearningMechanism.SVM:
        time_start = perf_counter()
        last_measurement = svm.decision_function([frequencies_vectors[-1]])
        time_stop = perf_counter()
        if last_measurement < 0:
            syslog(LOG_WARNING, f'Обнаружено аномально большое количество пакетов. '
                                f'Время анализа: {time_stop - time_start} скекунд')


def process_pack(pack: Packet):
    time = datetime.now()
    if pack.haslayer(KRB_TGS_REQ):
        req = pack[KRB_TGS_REQ]
    elif pack.haslayer(KRB_AS_REQ):
        req = pack[KRB_AS_REQ]
    else:
        syslog(LOG_INFO, 'Обнаружен неопознанный пакет, содержащий KRB-KDC-REQ-BODY. Пропускаем')
        return
    krb_kdc_req_body = pack[KRB_KDC_REQ_BODY]
    cname = krb_kdc_req_body.cname
    cname_str = None if cname is None else '/'.join(['' if part is None else part.val.decode() for part in cname.nameString])
    sname_str = '/'.join(['' if part is None else part.val.decode() for part in krb_kdc_req_body.sname.nameString])
    source = pack[IP].src
    realm = krb_kdc_req_body.realm.val.decode()
    syslog(LOG_INFO, f'Получен {req.summary()}. Отправитель: {source} '
                     f'(клиент {cname_str}). '
                     f'Запрашиваемый сервис: {sname_str}. '
                     f'Realm: {krb_kdc_req_body.realm.val.decode()}')
    records.append(Record(time, req.summary(), source, cname_str, sname_str, realm, None))
    time_without_seconds = time.replace(microsecond=0)
    if time_without_seconds in frequencies.keys():
        frequencies[time_without_seconds].number += 1
        frequencies[time_without_seconds].source_sname_pairs.add((source, sname_str))
    else:
        frequencies[time_without_seconds] = FrequenciesItem(1, {(source, sname_str)}, 0)
    # обнаруживаем шифрование слабым алгоритмом RC4 (etype = 0x17 = 23)
    # находим алгоритм шифрования у TGT
    krb_ticket_etype = req[KRB_Ticket][EncryptedData].etype.val \
        if req.haslayer(KRB_Ticket) and req[KRB_Ticket].haslayer(EncryptedData) else None
    # находим алгоритм шифрования во вложенном AP_REQ
    krb_ap_req_etype = req[KRB_AP_REQ][EncryptedData].etype.val \
        if req.haslayer(KRB_AP_REQ) and req[KRB_AP_REQ].haslayer(EncryptedData) else None
    # находим алгоритм шифрования в PADATA
    padata_etype = req[PADATA][EncryptedData].etype.val \
        if req.haslayer(PADATA) and req[PADATA].haslayer(EncryptedData) else None
    if (krb_ticket_etype == rc4_hmac_signature or krb_ap_req_etype == rc4_hmac_signature or
            padata_etype == rc4_hmac_signature):
        syslog(LOG_WARNING, 'Обнаружен пакет, зашифрованный алгоритмом RC4.')


if __name__ == '__main__':
    signal(SIGTERM, handle_sigterm)
    parser = ArgumentParser(description='Детектор атаки Kerberoasting')
    parser.add_argument('--iface', required=False, help='Отслеживаемый интерфейс')
    parser.add_argument('--mech', type=lambda mech: LearningMechanism[mech], default=LearningMechanism.SVM,
                        choices=list(LearningMechanism), help='Механизм обнаружения')
    args = parser.parse_args()
    records = []
    mech = args.mech
    frequencies = load_dataset()
    every().day.at('00:00').do(dump_dataset)
    every().second.do(find_anomaly)
    scheduler = Thread(target=run_schedule)
    scheduler.start()
    iface = args.iface
    syslog(LOG_INFO, f'Детектор атаки Kerberoasting запущен.')
    syslog(LOG_INFO, f'Запущено прослушивание интерфейса {iface}.')
    sniff(lfilter=lambda pack: pack.haslayer(KRB_KDC_REQ_BODY),
          prn=lambda pack: process_pack(pack),
          iface=iface)
